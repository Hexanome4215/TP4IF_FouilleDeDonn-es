\subsection{\eng{Clustering}}
Une fois la phase de sélection des données, il reste évidemment a effectuer le \eng{clustering}. Cette étape consiste à classifier les données en paquet afin de pouvoir en extraire des relations entre eux.

Cette étape se réalise grâce à de nombreux algorithmes. Profitons de cette partie pour étudier les spécificité de 3 grands algorithmes : 
\begin{itemize}
	\item Le \eng{clustering} hiérarchique
	\item La méthode \eng{K-Mean}
	\item La méthode \eng{Fuzzy C-Mean}
\end{itemize}

\subsubsection{Détermination du nombre de \eng{clusters}}
Le nombre de \eng{clusters} n'est pas laissé libre aux différents algorithmes et il faut prendre une décision cohérente.

Parfois, ce nombre est fixé : \eg lorsque le client commande une classification en 3 groupes \og maigre\fg, \og normal\fg, \og obèse\fg.

Cependant, ce n'est pas toujours (et même rarement) le cas. Il existe cependant une méthode pour déterminer de manière formelle un nombre de cluster adapté au jeu de données.

\paragraph{\eng{Clustering} hiérarchique}
Encore une fois, l'utilisation du \eng{clustering} hiérarchique est recommandé pour prendre des décisions.

L'idée est que le nombre de cluster correspond au nombre de \eng{gaps} de la courbe de distance : \image{Distance}{0.7}
Dans ce cas, il apparait que 3 \eng{clusters} serait un bon choix.

Il faut cependant bien garder à l'esprit que le nombre de \eng{gaps} est fortement dépendant du type de fusion utilisé pour le \eng{clustering}. Knime propose 3 types de fusion :
\begin{itemize}
	\item \texttt{SINGLE}\footnote{Appelé aussi \texttt{Nearest Neighbour}, ou encore \texttt{Shortest Distance}} : Établit la distance entre deux \eng{clusters} comme étant la plus petite distance entres deux points des \eng{clusters}.
	\item \texttt{AVERAGE} : Établit la distance entre deux \eng{clusters} comme étant la distance entre les centroids des clusters. 
	\item \texttt{COMPLETE} : Établit la distance entre deux \eng{clusters} comme étant le plus grande distance entre deux points des \eng{clusters}.
\end{itemize}

Par conséquent, le choix entre ces différentes fonction de distance s'effectue en fonction de la distribution des données. En effet, en \texttt{SINGLE} ou \texttt{COMPLETE}, les extrémaux auront énormément d'influence sur le \eng{clustering}. En \texttt{AVERAGE} par contre, ils auront \apriori pas d'influence.

\subsection{\eng{Clustering}}
Une fois les données \og nettoyées\fg~et le nombre de \eng{clusters} déterminé, il est enfin temps de procéder au \eng{clustering} (\textsl{enfin !}).

Nous pouvons dors et déjà distinguer deux types de \eng{clustering} :
\todo{}
\begin{itemize}
	\item Le \eng{clustering} hiérarchique :
	\item Le \eng{clustering} partitionnel :
\end{itemize}

Nous n'avons pas d'intérêt dans ce document à détailler le fonctionnement interne de ces algorithmes, cependant, il est intéressant de voir dans quel cas nous choisirons l'un ou l'autre.\\

\clusterAlgorithm
{K-Mean}
{Partitionnel}
{
	\item Accepte un \eng{dataset} relativement grand.
	\item Rapide.
	\item Donne une appartenance binaire (\ie \og appartient\fg~ou \og n'appartient pas\fg~au cluster).
}
{
	\item Sensible aux conditions initiales, il nécessite donc une vérification de la stabilité.
}

\clusterAlgorithm
{Fuzzy C-Mean}
{Partitionnel}
{
	\item Accepte un \eng{dataset} relativement grand.
	\item Rapide.
	\item Donne une appartenance réelle (\ie le pourcentage d'appartenance à chaque cluster) au cluster.
}
{
	\item Sensible aux conditions initiales, il nécessite donc une vérification de la stabilité.
}

\subsubsection{Normalisation}
La question de la normalisation est fondamentale dans le sens qu'elle influence le résultat donné par les algorithmes de \eng{clustering} du même type que \eng{K-Mean}. En effet, \eng{K-Mean} ne normalise pas automatiquement les données et la magnitude de celles-ci peuvent alors influencer ce \eng{clustering}.

Nous disponsong de plusieurs types de normalisation :
\todo{}
\begin{itemize}
	\item [Min-Max] 
	\item [Z-Score]
	\item [Decimal Scaling] 
\end{itemize}

Elle est cependant inutile dans le cas de valeurs relatives (\eg du type pourcentages).

\subsection{Conclusion}
Bien que cette partie résumant notre démarche expérimentale pourrait ressembler à un résumé du cours et des mes lectures sur le sujet, il nous a permit d'une part d'aborder des questions épineuses comme la normalisation, le manques de valeurs ou encore la malédiction du clustering, et ceux, en détail. De plus, il nous a semblé cohérent que, à la vue d'un sujet aussi libre, nous nous devions de justifier nos résultats par l'application d'une méthode rigoureuse.
