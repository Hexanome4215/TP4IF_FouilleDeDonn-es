\subsection{\eng{Clustering}}
Une fois la phase de sélection des données, il reste évidemment a effectuer le \eng{clustering}. Cette étape consiste à classifier les données en paquet afin de pouvoir en extraire des relations entre eux.

Cette étape se réalise grâce à de nombreux algorithmes. Profitons de cette partie pour étudier les spécificité de 3 grands algorithmes : 
\begin{itemize}
	\item Le \eng{clustering} hiérarchique
	\item La méthode \eng{K-Mean}
	\item La méthode \eng{Fuzzy C-Mean}
\end{itemize}

\subsubsection{Détermination du nombre de \eng{clusters}}
Le nombre de \eng{clusters} n'est pas laissé libre aux différents algorithmes et il faut prendre une décision cohérente.

Parfois, ce nombre est fixé : \eg lorsque le client commande une classification en 3 groupes \og maigre\fg, \og normal\fg, \og obèse\fg.

Cependant, ce n'est pas toujours (et même rarement) le cas. Il existe cependant une méthode pour déterminer de manière formelle un nombre de cluster adapté au jeu de données.

\paragraph{\eng{Clustering} hiérarchique}
Encore une fois, l'utilisation du \eng{clustering} hiérarchique est recommandé pour prendre des décisions.

L'idée est que le nombre de cluster correspond au nombre de \eng{gaps} de la courbe de distance : \image{Distance}{0.7}
Dans ce cas, il apparait que 3 \eng{clusters} serait un bon choix.

Il faut cependant bien garder à l'esprit que le nombre de \eng{gaps} est fortement dépendant du type de fusion utilisé pour le \eng{clustering}. Knime propose 3 types de fusion :
\begin{itemize}
	\item \texttt{SINGLE}\footnote{Appelé aussi \texttt{Nearest Neighbour}, ou encore \texttt{Shortest Distance}} : Établit la distance entre deux \eng{clusters} comme étant la plus petite distance entres deux points des \eng{clusters}.
	\item \texttt{AVERAGE} : Établit la distance entre deux \eng{clusters} comme étant la distance entre les centroids des clusters. 
	\item \texttt{COMPLETE} : Établit la distance entre deux \eng{clusters} comme étant le plus grande distance entre deux points des \eng{clusters}.
\end{itemize}

Par conséquent, le choix entre ces différentes fonction de distance s'effectue en fonction de la distribution des données. En effet, en \texttt{SINGLE} ou \texttt{COMPLETE}, les extrémaux auront énormément d'influence sur le \eng{clustering}. En \texttt{AVERAGE} par contre, ils auront \apriori pas d'influence.

\subsection{\eng{Clustering}}
Une fois les données \og nettoyées\fg~et le nombre de \eng{clusters} déterminé, il est enfin temps de procéder au \eng{clustering} (\textsl{enfin !}).

Nous pouvons dors et déjà distinguer deux types de \eng{clustering} :
\begin{itemize}
	\item Le \eng{clustering} hiérarchique :
	\item Le \eng{clustering} partitionnel :
\end{itemize}

\subsubsection{\eng{Hierarchical Clustering}}
\paragraph{Arbre de décision}

\subsubsection{\eng{Clustering} partitionnel}
\paragraph{\eng{K-Mean}}

\paragraph {\eng{Fuzzy C-Mean}} 

\subsubsection{Normalisation}
\todo{Elle est parfois inutile (dans le cas de valeur en pourcentages \eg}
